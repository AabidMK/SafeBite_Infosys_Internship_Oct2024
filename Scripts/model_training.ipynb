{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecfb0c6-1fe4-4bc7-8afe-3f86a26a81d4",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "021b448c-c515-4de6-bdc5-f81c76bb08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b8c1d-63c7-4a62-a1d5-d6e8855eeb2c",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1822057a-a282-4e6a-aafa-6c1938862765",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Dataset/SafeBite_preprocessed_data_loo_enc_price.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2c0b078-c178-4e09-800e-df3a5a731ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split the dataset into features (X) and target variable (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8e58b4c-7f2c-44f2-8f50-6dceccfd41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (399, 8)\n",
      "Shape of Y: (399,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into features (X) and target variable (Y)\n",
    "X = data.drop(columns=[\"Is_Allergen\"])\n",
    "Y = data[\"Is_Allergen\"]\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680c855-71f3-44d9-8b6c-87832e1416b6",
   "metadata": {},
   "source": [
    "#### Perform an 80:20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23ef2877-20db-4720-8bf5-88e22f705b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_80, X_test_80, Y_train_80, Y_test_80 = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3a02967-0434-407d-b5a2-1a1260aa2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80:20 Split:\n",
      "X_train_80 shape: (319, 8)\n",
      "X_test_80 shape: (80, 8)\n",
      "Y_train_80 shape: (319,)\n",
      "Y_test_80 shape: (80,)\n"
     ]
    }
   ],
   "source": [
    "#Print the shape of the 80:20 split data\n",
    "print(\"\\n80:20 Split:\")\n",
    "print(\"X_train_80 shape:\", X_train_80.shape)\n",
    "print(\"X_test_80 shape:\", X_test_80.shape)\n",
    "print(\"Y_train_80 shape:\", Y_train_80.shape)\n",
    "print(\"Y_test_80 shape:\", Y_test_80.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183317ad-da33-4be4-8fa7-07d8e1ba7837",
   "metadata": {},
   "source": [
    "#### Perform an 70:30 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6ebe668-ef50-4682-afbe-962e3d4e92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_70, X_test_70, Y_train_70, Y_test_70 = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f7317b4-59fa-40f1-93c4-9c1331a36d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "70:30 Split:\n",
      "X_train_70 shape: (279, 8)\n",
      "X_test_70 shape: (120, 8)\n",
      "Y_train_70 shape: (279,)\n",
      "Y_test_70 shape: (120,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the 70:30 split data\n",
    "print(\"\\n70:30 Split:\")\n",
    "print(\"X_train_70 shape:\", X_train_70.shape)\n",
    "print(\"X_test_70 shape:\", X_test_70.shape)\n",
    "print(\"Y_train_70 shape:\", Y_train_70.shape)\n",
    "print(\"Y_test_70 shape:\", Y_test_70.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b9343-b509-463c-968f-9c91ddde7bff",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37c9ccee-ad68-4313-93ac-1194267d0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a337311-7da7-4cba-9fa6-e577259dd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on 80:20 split\n",
    "logreg.fit(X_train_80, Y_train_80)\n",
    "logreg_train_pred_80 = logreg.predict(X_train_80)\n",
    "logreg_test_pred_80 = logreg.predict(X_test_80)\n",
    "# Train and evaluate on 70:30 split\n",
    "logreg.fit(X_train_70, Y_train_70)\n",
    "logreg_train_pred_70 = logreg.predict(X_train_70)\n",
    "logreg_test_pred_70 = logreg.predict(X_test_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea231009-98e0-4ce1-860f-90d586c749b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics 80:20 split\n",
    "logreg_train_acc_80 = accuracy_score(Y_train_80, logreg_train_pred_80)\n",
    "logreg_test_acc_80 = accuracy_score(Y_test_80, logreg_test_pred_80)\n",
    "logreg_precision_80 = precision_score(Y_test_80, logreg_test_pred_80)\n",
    "logreg_recall_80 = recall_score(Y_test_80, logreg_test_pred_80)\n",
    "logreg_f1_80 = f1_score(Y_test_80, logreg_test_pred_80)\n",
    "\n",
    "# Calculate metrics 70:30 split\n",
    "logreg_train_acc_70 = accuracy_score(Y_train_70, logreg_train_pred_70)\n",
    "logreg_test_acc_70 = accuracy_score(Y_test_70, logreg_test_pred_70)\n",
    "logreg_precision_70 = precision_score(Y_test_70, logreg_test_pred_70)\n",
    "logreg_recall_70 = recall_score(Y_test_70, logreg_test_pred_70)\n",
    "logreg_f1_70 = f1_score(Y_test_70, logreg_test_pred_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aeb68894-e96f-4b80-bedd-e338a95543af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (80:20 split):\n",
      "Accuracy: 0.65\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.8620689655172413\n",
      "F1 Score: 0.78125\n",
      "\n",
      "Logistic Regression (70:30 split):\n",
      "Accuracy: 0.6083333333333333\n",
      "Precision: 0.7\n",
      "Recall: 0.8045977011494253\n",
      "F1 Score: 0.7486631016042781\n"
     ]
    }
   ],
   "source": [
    "# Print results for Logistic Regression\n",
    "print(\"Logistic Regression (80:20 split):\")\n",
    "print(\"Accuracy:\", logreg_test_acc_80)\n",
    "print(\"Precision:\", logreg_precision_80)\n",
    "print(\"Recall:\", logreg_recall_80)\n",
    "print(\"F1 Score:\", logreg_f1_80)\n",
    "print(\"\\nLogistic Regression (70:30 split):\")\n",
    "print(\"Accuracy:\", logreg_test_acc_70)\n",
    "print(\"Precision:\", logreg_precision_70)\n",
    "print(\"Recall:\", logreg_recall_70)\n",
    "print(\"F1 Score:\", logreg_f1_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49dac4-d908-4ea0-84aa-0e32469c0455",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ef3dda3-7125-4ad0-a754-56d0c4693f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest\n",
    "random_forest = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "010685ec-e439-4c4c-8aae-2b6574a3bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on 80:20 split\n",
    "random_forest.fit(X_train_80, Y_train_80)\n",
    "rf_train_pred_80 = random_forest.predict(X_train_80)\n",
    "rf_test_pred_80 = random_forest.predict(X_test_80)\n",
    "# Train and evaluate for 70:30 split\n",
    "random_forest.fit(X_train_70, Y_train_70)\n",
    "rf_train_pred_70 = random_forest.predict(X_train_70)\n",
    "rf_test_pred_70 = random_forest.predict(X_test_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6fa9c4d-e081-478d-b432-495114e05850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics 80:20 split\n",
    "rf_train_acc_80 = accuracy_score(Y_train_80, rf_train_pred_80)\n",
    "rf_test_acc_80 = accuracy_score(Y_test_80, rf_test_pred_80)\n",
    "rf_precision_80 = precision_score(Y_test_80, rf_test_pred_80)\n",
    "rf_recall_80 = recall_score(Y_test_80, rf_test_pred_80)\n",
    "rf_f1_80 = f1_score(Y_test_80, rf_test_pred_80)\n",
    "\n",
    "# Calculate metrics 70:30 split\n",
    "rf_train_acc_70 = accuracy_score(Y_train_70, rf_train_pred_70)\n",
    "rf_test_acc_70 = accuracy_score(Y_test_70, rf_test_pred_70)\n",
    "rf_precision_70 = precision_score(Y_test_70, rf_test_pred_70)\n",
    "rf_recall_70 = recall_score(Y_test_70, rf_test_pred_70)\n",
    "rf_f1_70 = f1_score(Y_test_70, rf_test_pred_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39057703-e813-4234-8aea-5681d5f45e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (80:20 split):\n",
      "Accuracy: 0.9875\n",
      "Precision: 1.0\n",
      "Recall: 0.9827586206896551\n",
      "F1 Score: 0.991304347826087\n",
      "\n",
      "Random Forest (70:30 split):\n",
      "Accuracy: 0.975\n",
      "Precision: 0.9772727272727273\n",
      "Recall: 0.9885057471264368\n",
      "F1 Score: 0.9828571428571429\n"
     ]
    }
   ],
   "source": [
    "# Print results for Random Forest\n",
    "print(\"\\nRandom Forest (80:20 split):\")\n",
    "print(\"Accuracy:\", rf_test_acc_80)\n",
    "print(\"Precision:\", rf_precision_80)\n",
    "print(\"Recall:\", rf_recall_80)\n",
    "print(\"F1 Score:\", rf_f1_80)\n",
    "print(\"\\nRandom Forest (70:30 split):\")\n",
    "print(\"Accuracy:\", rf_test_acc_70)\n",
    "print(\"Precision:\", rf_precision_70)\n",
    "print(\"Recall:\", rf_recall_70)\n",
    "print(\"F1 Score:\", rf_f1_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1475260-7205-4be7-bc79-dbb95e42ac46",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1b2b864-5ed6-4c9f-87fe-266d81a49275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AdaBoost\n",
    "adaboost = AdaBoostClassifier(algorithm='SAMME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15867c91-d695-42d2-8f71-acc2a99382ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on 80:20 split\n",
    "adaboost.fit(X_train_80, Y_train_80)\n",
    "adaboost_train_pred_80 = adaboost.predict(X_train_80)\n",
    "adaboost_test_pred_80 = adaboost.predict(X_test_80)\n",
    "# Train and evaluate on 70:30 split\n",
    "adaboost.fit(X_train_70, Y_train_70)\n",
    "adaboost_train_pred_70 = adaboost.predict(X_train_70)\n",
    "adaboost_test_pred_70 = adaboost.predict(X_test_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b854780-4622-4dc7-be34-bb134535086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics 80:20 split\n",
    "adaboost_train_acc_80 = accuracy_score(Y_train_80, adaboost_train_pred_80)\n",
    "adaboost_test_acc_80 = accuracy_score(Y_test_80, adaboost_test_pred_80)\n",
    "adaboost_precision_80 = precision_score(Y_test_80, adaboost_test_pred_80)\n",
    "adaboost_recall_80 = recall_score(Y_test_80, adaboost_test_pred_80)\n",
    "adaboost_f1_80 = f1_score(Y_test_80, adaboost_test_pred_80)\n",
    "\n",
    "# Calculate metrics 70:30 split\n",
    "adaboost_train_acc_70 = accuracy_score(Y_train_70, adaboost_train_pred_70)\n",
    "adaboost_test_acc_70 = accuracy_score(Y_test_70, adaboost_test_pred_70)\n",
    "adaboost_precision_70 = precision_score(Y_test_70, adaboost_test_pred_70)\n",
    "adaboost_recall_70 = recall_score(Y_test_70, adaboost_test_pred_70)\n",
    "adaboost_f1_70 = f1_score(Y_test_70, adaboost_test_pred_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93288acf-363c-4bb8-ac9c-7eecbc6a4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost (80:20 split):\n",
      "Accuracy: 0.9875\n",
      "Precision: 1.0\n",
      "Recall: 0.9827586206896551\n",
      "F1 Score: 0.991304347826087\n",
      "\n",
      "AdaBoost (70:30 split):\n",
      "Accuracy: 0.9666666666666667\n",
      "Precision: 0.9770114942528736\n",
      "Recall: 0.9770114942528736\n",
      "F1 Score: 0.9770114942528736\n"
     ]
    }
   ],
   "source": [
    "# Print results for AdaBoost\n",
    "print(\"\\nAdaBoost (80:20 split):\")\n",
    "print(\"Accuracy:\", adaboost_test_acc_80)\n",
    "print(\"Precision:\", adaboost_precision_80)\n",
    "print(\"Recall:\", adaboost_recall_80)\n",
    "print(\"F1 Score:\", adaboost_f1_80)\n",
    "print(\"\\nAdaBoost (70:30 split):\")\n",
    "print(\"Accuracy:\", adaboost_test_acc_70)\n",
    "print(\"Precision:\", adaboost_precision_70)\n",
    "print(\"Recall:\", adaboost_recall_70)\n",
    "print(\"F1 Score:\", adaboost_f1_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eada1a3-25aa-4a73-a266-8a5a59548523",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b1926ba-fec5-484f-ba0f-c120591b4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42c9163d-e6a2-405e-beb8-f922fcc235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on 80:20 split\n",
    "decision_tree.fit(X_train_80, Y_train_80)\n",
    "dt_train_pred_80 = decision_tree.predict(X_train_80)\n",
    "dt_test_pred_80 = decision_tree.predict(X_test_80)\n",
    "# Train and evaluate on 70:30 split\n",
    "decision_tree.fit(X_train_70, Y_train_70)\n",
    "dt_train_pred_70 = decision_tree.predict(X_train_70)\n",
    "dt_test_pred_70 = decision_tree.predict(X_test_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "567150a3-e1ce-438b-95ed-1e25e82e41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics 80:20 split\n",
    "dt_train_acc_80 = accuracy_score(Y_train_80, dt_train_pred_80)\n",
    "dt_test_acc_80 = accuracy_score(Y_test_80, dt_test_pred_80)\n",
    "dt_precision_80 = precision_score(Y_test_80, dt_test_pred_80)\n",
    "dt_recall_80 = recall_score(Y_test_80, dt_test_pred_80)\n",
    "dt_f1_80 = f1_score(Y_test_80, dt_test_pred_80)\n",
    "\n",
    "# Calculate metrics 80:20 split\n",
    "dt_train_acc_70 = accuracy_score(Y_train_70, dt_train_pred_70)\n",
    "dt_test_acc_70 = accuracy_score(Y_test_70, dt_test_pred_70)\n",
    "dt_precision_70 = precision_score(Y_test_70, dt_test_pred_70)\n",
    "dt_recall_70 = recall_score(Y_test_70, dt_test_pred_70)\n",
    "dt_f1_70 = f1_score(Y_test_70, dt_test_pred_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "352c4aa5-1af6-4255-8d27-6a5beca8e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (80:20 split):\n",
      "Accuracy: 0.9375\n",
      "Precision: 0.9818181818181818\n",
      "Recall: 0.9310344827586207\n",
      "F1 Score: 0.9557522123893806\n",
      "\n",
      "Decision Tree (70:30 split):\n",
      "Accuracy: 0.975\n",
      "Precision: 1.0\n",
      "Recall: 0.9655172413793104\n",
      "F1 Score: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "# Print results for Decision Tree\n",
    "print(\"\\nDecision Tree (80:20 split):\")\n",
    "print(\"Accuracy:\", dt_test_acc_80)\n",
    "print(\"Precision:\", dt_precision_80)\n",
    "print(\"Recall:\", dt_recall_80)\n",
    "print(\"F1 Score:\", dt_f1_80)\n",
    "print(\"\\nDecision Tree (70:30 split):\")\n",
    "print(\"Accuracy:\", dt_test_acc_70)\n",
    "print(\"Precision:\", dt_precision_70)\n",
    "print(\"Recall:\", dt_recall_70)\n",
    "print(\"F1 Score:\", dt_f1_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d311734-bdde-49a5-9f91-1f1ebf7325be",
   "metadata": {},
   "source": [
    "###  XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12c4404c-ed1a-435f-bea0-f946e139baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgboost = XGBClassifier(eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff686896-8774-41bf-ab19-496ced123ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on 80:20 split\n",
    "xgboost.fit(X_train_80, Y_train_80)\n",
    "xgboost_train_pred_80 = xgboost.predict(X_train_80)\n",
    "xgboost_test_pred_80 = xgboost.predict(X_test_80)\n",
    "\n",
    "# Train and evaluate on 70:30 split\n",
    "xgboost.fit(X_train_70, Y_train_70)\n",
    "xgboost_train_pred_70 = xgboost.predict(X_train_70)\n",
    "xgboost_test_pred_70 = xgboost.predict(X_test_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ba9ab1a-9057-4b4a-bdc8-f61ad2138335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics 80:20 split\n",
    "xgboost_train_acc_80 = accuracy_score(Y_train_80, xgboost_train_pred_80)\n",
    "xgboost_test_acc_80 = accuracy_score(Y_test_80, xgboost_test_pred_80)\n",
    "xgboost_precision_80 = precision_score(Y_test_80, xgboost_test_pred_80)\n",
    "xgboost_recall_80 = recall_score(Y_test_80, xgboost_test_pred_80)\n",
    "xgboost_f1_80 = f1_score(Y_test_80, xgboost_test_pred_80)\n",
    "\n",
    "# Calculate metrics 70:30 split\n",
    "xgboost_train_acc_70 = accuracy_score(Y_train_70, xgboost_train_pred_70)\n",
    "xgboost_test_acc_70 = accuracy_score(Y_test_70, xgboost_test_pred_70)\n",
    "xgboost_precision_70 = precision_score(Y_test_70, xgboost_test_pred_70)\n",
    "xgboost_recall_70 = recall_score(Y_test_70, xgboost_test_pred_70)\n",
    "xgboost_f1_70 = f1_score(Y_test_70, xgboost_test_pred_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ea95ce1-17c4-4e1d-a30c-850e377fed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost (80:20 split):\n",
      "Accuracy: 0.9875\n",
      "Precision: 1.0\n",
      "Recall: 0.9827586206896551\n",
      "F1 Score: 0.991304347826087\n",
      "\n",
      "XGBoost (70:30 split):\n",
      "Accuracy: 0.9916666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.9885057471264368\n",
      "F1 Score: 0.9942196531791907\n"
     ]
    }
   ],
   "source": [
    "# Print results for XGBoost\n",
    "print(\"\\nXGBoost (80:20 split):\")\n",
    "print(\"Accuracy:\", xgboost_test_acc_80)\n",
    "print(\"Precision:\", xgboost_precision_80)\n",
    "print(\"Recall:\", xgboost_recall_80)\n",
    "print(\"F1 Score:\", xgboost_f1_80)\n",
    "print(\"\\nXGBoost (70:30 split):\")\n",
    "print(\"Accuracy:\", xgboost_test_acc_70)\n",
    "print(\"Precision:\", xgboost_precision_70)\n",
    "print(\"Recall:\", xgboost_recall_70)\n",
    "print(\"F1 Score:\", xgboost_f1_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ac619-078a-4182-b543-4963428a2d78",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6643d8d1-e1c7-4ca1-acf2-f9348e01f182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN (80:20 split):\n",
      "Accuracy: 0.7125\n",
      "Precision: 0.8571428571428571\n",
      "Recall: 0.7241379310344828\n",
      "F1 Score: 0.7850467289719626\n",
      "\n",
      "KNN (70:30 split):\n",
      "Accuracy: 0.6416666666666667\n",
      "Precision: 0.8666666666666667\n",
      "Recall: 0.5977011494252874\n",
      "F1 Score: 0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "# Initialize KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train and evaluate on 80:20 split\n",
    "knn.fit(X_train_80, Y_train_80)\n",
    "knn_train_pred_80 = knn.predict(X_train_80)\n",
    "knn_test_pred_80 = knn.predict(X_test_80)\n",
    "\n",
    "# Calculate metrics\n",
    "knn_train_acc_80 = accuracy_score(Y_train_80, knn_train_pred_80)\n",
    "knn_test_acc_80 = accuracy_score(Y_test_80, knn_test_pred_80)\n",
    "knn_precision_80 = precision_score(Y_test_80, knn_test_pred_80)\n",
    "knn_recall_80 = recall_score(Y_test_80, knn_test_pred_80)\n",
    "knn_f1_80 = f1_score(Y_test_80, knn_test_pred_80)\n",
    "\n",
    "# Train and evaluate on 70:30 split\n",
    "knn.fit(X_train_70, Y_train_70)\n",
    "knn_train_pred_70 = knn.predict(X_train_70)\n",
    "knn_test_pred_70 = knn.predict(X_test_70)\n",
    "\n",
    "# Calculate metrics\n",
    "knn_train_acc_70 = accuracy_score(Y_train_70, knn_train_pred_70)\n",
    "knn_test_acc_70 = accuracy_score(Y_test_70, knn_test_pred_70)\n",
    "knn_precision_70 = precision_score(Y_test_70, knn_test_pred_70)\n",
    "knn_recall_70 = recall_score(Y_test_70, knn_test_pred_70)\n",
    "knn_f1_70 = f1_score(Y_test_70, knn_test_pred_70)\n",
    "\n",
    "# Print results for K-Nearest Neighbors (KNN)\n",
    "print(\"\\nKNN (80:20 split):\")\n",
    "print(\"Accuracy:\", knn_test_acc_80)\n",
    "print(\"Precision:\", knn_precision_80)\n",
    "print(\"Recall:\", knn_recall_80)\n",
    "print(\"F1 Score:\", knn_f1_80)\n",
    "print(\"\\nKNN (70:30 split):\")\n",
    "print(\"Accuracy:\", knn_test_acc_70)\n",
    "print(\"Precision:\", knn_precision_70)\n",
    "print(\"Recall:\", knn_recall_70)\n",
    "print(\"F1 Score:\", knn_f1_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d133c86e-3151-47e9-ae42-57d0b9d73d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy (80:20)</th>\n",
       "      <th>Test Accuracy (80:20)</th>\n",
       "      <th>Train Accuracy (70:30)</th>\n",
       "      <th>Test Accuracy (70:30)</th>\n",
       "      <th>Total Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.589342</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.594982</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.610664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.990625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.994792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.981191</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.982079</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.979359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.978125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.711599</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.699237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Accuracy (80:20)  Test Accuracy (80:20)  \\\n",
       "0  Logistic Regression                0.589342                 0.6500   \n",
       "1        Random Forest                1.000000                 0.9875   \n",
       "2              XGBoost                1.000000                 0.9875   \n",
       "3             AdaBoost                0.981191                 0.9875   \n",
       "4        Decision Tree                1.000000                 0.9375   \n",
       "5                  KNN                0.711599                 0.7125   \n",
       "\n",
       "   Train Accuracy (70:30)  Test Accuracy (70:30)  Total Accuracy  \n",
       "0                0.594982               0.608333        0.610664  \n",
       "1                1.000000               0.975000        0.990625  \n",
       "2                1.000000               0.991667        0.994792  \n",
       "3                0.982079               0.966667        0.979359  \n",
       "4                1.000000               0.975000        0.978125  \n",
       "5                0.731183               0.641667        0.699237  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for the table: Model, Train Accuracy (80:20), Test Accuracy (80:20), Train Accuracy (70:30), Test Accuracy (70:30), Total Accuracy\n",
    "model_comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'AdaBoost', 'Decision Tree', 'KNN'],\n",
    "    'Train Accuracy (80:20)': [logreg_train_acc_80, rf_train_acc_80, xgboost_train_acc_80, adaboost_train_acc_80, dt_train_acc_80, knn_train_acc_80],\n",
    "    'Test Accuracy (80:20)': [logreg_test_acc_80, rf_test_acc_80, xgboost_test_acc_80, adaboost_test_acc_80, dt_test_acc_80, knn_test_acc_80],\n",
    "    'Train Accuracy (70:30)': [logreg_train_acc_70, rf_train_acc_70, xgboost_train_acc_70, adaboost_train_acc_70, dt_train_acc_70, knn_train_acc_70],\n",
    "    'Test Accuracy (70:30)': [logreg_test_acc_70, rf_test_acc_70, xgboost_test_acc_70, adaboost_test_acc_70, dt_test_acc_70, knn_test_acc_70],\n",
    "    'Total Accuracy': [\n",
    "        (logreg_train_acc_80 + logreg_test_acc_80 + logreg_train_acc_70 + logreg_test_acc_70) / 4,\n",
    "        (rf_train_acc_80 + rf_test_acc_80 + rf_train_acc_70 + rf_test_acc_70) / 4,\n",
    "        (xgboost_train_acc_80 + xgboost_test_acc_80 + xgboost_train_acc_70 + xgboost_test_acc_70) / 4,\n",
    "        (adaboost_train_acc_80 + adaboost_test_acc_80 + adaboost_train_acc_70 + adaboost_test_acc_70) / 4,\n",
    "        (dt_train_acc_80 + dt_test_acc_80 + dt_train_acc_70 + dt_test_acc_70) / 4,\n",
    "        (knn_train_acc_80 + knn_test_acc_80 + knn_train_acc_70 + knn_test_acc_70) / 4\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "model_comparison_df = pd.DataFrame(model_comparison_data)\n",
    "\n",
    "# Display the comparison table\n",
    "model_comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9e711d3-a9a9-428f-81a3-87aa586b6a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Better split is 80:20\n",
      "Random Forest: Better split is 80:20\n",
      "AdaBoost: Better split is 80:20\n",
      "Decision Tree: Better split is 70:30\n",
      "XGBoost: Better split is 70:30\n",
      "KNN: Better split is 80:20\n"
     ]
    }
   ],
   "source": [
    "# Example structure to store performance metrics for all models\n",
    "results = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"80:20\": {\"Accuracy\": logreg_test_acc_80, \"Precision\": logreg_precision_80, \"Recall\": logreg_recall_80, \"F1\": logreg_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": logreg_test_acc_70, \"Precision\": logreg_precision_70, \"Recall\": logreg_recall_70, \"F1\": logreg_f1_70}\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"80:20\": {\"Accuracy\": rf_test_acc_80, \"Precision\": rf_precision_80, \"Recall\": rf_recall_80, \"F1\": rf_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": rf_test_acc_70, \"Precision\": rf_precision_70, \"Recall\": rf_recall_70, \"F1\": rf_f1_70}\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"80:20\": {\"Accuracy\": adaboost_test_acc_80, \"Precision\": adaboost_precision_80, \"Recall\": adaboost_recall_80, \"F1\": adaboost_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": adaboost_test_acc_70, \"Precision\": adaboost_precision_70, \"Recall\": adaboost_recall_70, \"F1\": adaboost_f1_70}\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"80:20\": {\"Accuracy\": dt_test_acc_80, \"Precision\": dt_precision_80, \"Recall\": dt_recall_80, \"F1\": dt_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": dt_test_acc_70, \"Precision\": dt_precision_70, \"Recall\": dt_recall_70, \"F1\": dt_f1_70}\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"80:20\": {\"Accuracy\": xgboost_test_acc_80, \"Precision\": xgboost_precision_80, \"Recall\": xgboost_recall_80, \"F1\": xgboost_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": xgboost_test_acc_70, \"Precision\": xgboost_precision_70, \"Recall\": xgboost_recall_70, \"F1\": xgboost_f1_70}\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"80:20\": {\"Accuracy\": knn_test_acc_80, \"Precision\": knn_precision_80, \"Recall\": knn_recall_80, \"F1\": knn_f1_80},\n",
    "        \"70:30\": {\"Accuracy\": knn_test_acc_70, \"Precision\": knn_precision_70, \"Recall\": knn_recall_70, \"F1\": knn_f1_70}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Function to determine the better split\n",
    "def compare_splits(results):\n",
    "    better_split = {}\n",
    "    for model, metrics in results.items():\n",
    "        scores_80 = metrics[\"80:20\"]\n",
    "        scores_70 = metrics[\"70:30\"]\n",
    "        # Count metrics where 80:20 performs better and vice versa\n",
    "        better_80 = sum([scores_80[m] > scores_70[m] for m in scores_80])\n",
    "        better_70 = sum([scores_70[m] > scores_80[m] for m in scores_70])\n",
    "        better_split[model] = \"80:20\" if better_80 > better_70 else \"70:30\"\n",
    "    return better_split\n",
    "\n",
    "# Determine the better split for each model\n",
    "better_splits = compare_splits(results)\n",
    "\n",
    "# Print results\n",
    "for model, split in better_splits.items():\n",
    "    print(f\"{model}: Better split is {split}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6934f49-99cf-44c9-9499-afc29d6dac83",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51bfbf-b083-41f7-ba86-26da1ab079a6",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90c85828-88e1-412a-bd27-9203666f0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5b4e729-7ec8-469d-8b34-094111a04c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Random Forest Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        33\n",
      "           1       0.99      0.99      0.99        87\n",
      "\n",
      "    accuracy                           0.98       120\n",
      "   macro avg       0.98      0.98      0.98       120\n",
      "weighted avg       0.98      0.98      0.98       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_rf.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for Random Forest: {best_rf_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "Y_pred_rf = best_rf_model.predict(X_test_70)\n",
    "print(f\"Random Forest Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffc0a4-1762-4dd9-88d1-b7a658a44b36",
   "metadata": {},
   "source": [
    "### XGBoost Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df0178bc-06a0-42dc-8448-820161564591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "XGBoost Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        33\n",
      "           1       1.00      0.99      0.99        87\n",
      "\n",
      "    accuracy                           0.99       120\n",
      "   macro avg       0.99      0.99      0.99       120\n",
      "weighted avg       0.99      0.99      0.99       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the model\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_xgb.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for XGBoost: {best_xgb_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "Y_pred_xgb = best_xgb_model.predict(X_test_70)\n",
    "print(f\"XGBoost Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_xgb)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f1201-3391-46ea-b453-3adfc94b9406",
   "metadata": {},
   "source": [
    "###  Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5de4a78-58ba-4893-b185-f01f8440bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters for Logistic Regression: {'C': 0.01, 'solver': 'liblinear'}\n",
      "Logistic Regression Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.03      0.05        33\n",
      "           1       0.72      0.93      0.81        87\n",
      "\n",
      "    accuracy                           0.68       120\n",
      "   macro avg       0.43      0.48      0.43       120\n",
      "weighted avg       0.56      0.68      0.60       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_log_reg = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'saga']  # Solvers to use\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_log_reg = GridSearchCV(estimator=log_reg, param_grid=param_grid_log_reg, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_log_reg.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_log_reg_params = grid_search_log_reg.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for Logistic Regression: {best_log_reg_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_log_reg_model = grid_search_log_reg.best_estimator_\n",
    "Y_pred_log_reg = best_log_reg_model.predict(X_test_70)\n",
    "print(f\"Logistic Regression Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_log_reg)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4429c84-4616-4902-aec5-33a1f520037d",
   "metadata": {},
   "source": [
    "### AdaBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1e8308c-059d-4bbc-9e2a-b7e60efcbc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for AdaBoost: {'learning_rate': 1.0, 'n_estimators': 200}\n",
      "AdaBoost Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        33\n",
      "           1       0.99      0.99      0.99        87\n",
      "\n",
      "    accuracy                           0.98       120\n",
      "   macro avg       0.98      0.98      0.98       120\n",
      "weighted avg       0.98      0.98      0.98       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define the model\n",
    "ada_boost = AdaBoostClassifier(algorithm='SAMME')\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_ada_boost = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.1, 1.0]  # Learning rate\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_ada_boost = GridSearchCV(estimator=ada_boost, param_grid=param_grid_ada_boost, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_ada_boost.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_ada_boost_params = grid_search_ada_boost.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for AdaBoost: {best_ada_boost_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_ada_boost_model = grid_search_ada_boost.best_estimator_\n",
    "Y_pred_ada_boost = best_ada_boost_model.predict(X_test_70)\n",
    "print(f\"AdaBoost Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_ada_boost)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32abf76-8a76-4908-90f7-b8350bb58bf5",
   "metadata": {},
   "source": [
    "### Decision Tree Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9210bbd-4c5d-46f6-ab5d-cd6073e1ab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Decision Tree Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        33\n",
      "           1       1.00      0.98      0.99        87\n",
      "\n",
      "    accuracy                           0.98       120\n",
      "   macro avg       0.97      0.99      0.98       120\n",
      "weighted avg       0.98      0.98      0.98       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_dt.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_dt_params = grid_search_dt.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for Decision Tree: {best_dt_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "Y_pred_dt = best_dt_model.predict(X_test_70)\n",
    "print(f\"Decision Tree Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_dt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc330a-bf31-4074-a351-4e13e2f86b61",
   "metadata": {},
   "source": [
    "### KNN Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a20f3b5c-f016-467a-8673-302da4d64c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Best parameters for KNN: {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "KNN Classification Report (70:30 Split):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.64      0.48        33\n",
      "           1       0.82      0.62      0.71        87\n",
      "\n",
      "    accuracy                           0.62       120\n",
      "   macro avg       0.60      0.63      0.59       120\n",
      "weighted avg       0.70      0.62      0.64       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid_knn, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search_knn.fit(X_train_70, Y_train_70)\n",
    "\n",
    "# Best parameters from grid search\n",
    "best_knn_params = grid_search_knn.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters for KNN: {best_knn_params}\")\n",
    "\n",
    "# Evaluate on the test data\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "Y_pred_knn = best_knn_model.predict(X_test_70)\n",
    "print(f\"KNN Classification Report (70:30 Split):\\n{classification_report(Y_test_70, Y_pred_knn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e74d277-72da-4b2e-a116-31581c4c0925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy (Before Tuning)  Accuracy (After Tuning)\n",
      "0  Logistic Regression                  0.608333                 0.683333\n",
      "1        Random Forest                  0.983333                 0.983333\n",
      "2             AdaBoost                  0.966667                 0.983333\n",
      "3        Decision Tree                  0.950000                 0.983333\n",
      "4              XGBoost                  0.991667                 0.991667\n",
      "5                  KNN                  0.641667                 0.625000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# 1. Logistic Regression: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default Logistic Regression\n",
    "log_reg_default = LogisticRegression(max_iter=1000)\n",
    "log_reg_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_log_reg_default = log_reg_default.predict(X_test_70)\n",
    "acc_log_reg_default = accuracy_score(Y_test_70, y_pred_log_reg_default)\n",
    "\n",
    "# Hyperparameter tuned Logistic Regression\n",
    "best_log_reg_model = grid_search_log_reg.best_estimator_\n",
    "y_pred_log_reg_tuned = best_log_reg_model.predict(X_test_70)\n",
    "acc_log_reg_tuned = accuracy_score(Y_test_70, y_pred_log_reg_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['Logistic Regression', acc_log_reg_default, acc_log_reg_tuned])\n",
    "\n",
    "# 2. Random Forest: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default Random Forest\n",
    "rf_default = RandomForestClassifier()\n",
    "rf_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_rf_default = rf_default.predict(X_test_70)\n",
    "acc_rf_default = accuracy_score(Y_test_70, y_pred_rf_default)\n",
    "\n",
    "# Hyperparameter tuned Random Forest\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_rf_tuned = best_rf_model.predict(X_test_70)\n",
    "acc_rf_tuned = accuracy_score(Y_test_70, y_pred_rf_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['Random Forest', acc_rf_default, acc_rf_tuned])\n",
    "\n",
    "# 3. AdaBoost: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default AdaBoost\n",
    "ada_boost_default = AdaBoostClassifier(algorithm='SAMME')\n",
    "ada_boost_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_ada_boost_default = ada_boost_default.predict(X_test_70)\n",
    "acc_ada_boost_default = accuracy_score(Y_test_70, y_pred_ada_boost_default)\n",
    "\n",
    "# Hyperparameter tuned AdaBoost\n",
    "best_ada_boost_model = grid_search_ada_boost.best_estimator_\n",
    "y_pred_ada_boost_tuned = best_ada_boost_model.predict(X_test_70)\n",
    "acc_ada_boost_tuned = accuracy_score(Y_test_70, y_pred_ada_boost_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['AdaBoost', acc_ada_boost_default, acc_ada_boost_tuned])\n",
    "\n",
    "# 4. Decision Tree: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default Decision Tree\n",
    "dt_default = DecisionTreeClassifier()\n",
    "dt_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_dt_default = dt_default.predict(X_test_70)\n",
    "acc_dt_default = accuracy_score(Y_test_70, y_pred_dt_default)\n",
    "\n",
    "# Hyperparameter tuned Decision Tree\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "y_pred_dt_tuned = best_dt_model.predict(X_test_70)\n",
    "acc_dt_tuned = accuracy_score(Y_test_70, y_pred_dt_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['Decision Tree', acc_dt_default, acc_dt_tuned])\n",
    "\n",
    "# 5. XGBoost: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default XGBoost\n",
    "xgb_default = XGBClassifier()\n",
    "xgb_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_xgb_default = xgb_default.predict(X_test_70)\n",
    "acc_xgb_default = accuracy_score(Y_test_70, y_pred_xgb_default)\n",
    "\n",
    "# Hyperparameter tuned XGBoost\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "y_pred_xgb_tuned = best_xgb_model.predict(X_test_70)\n",
    "acc_xgb_tuned = accuracy_score(Y_test_70, y_pred_xgb_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['XGBoost', acc_xgb_default, acc_xgb_tuned])\n",
    "\n",
    "# 6. KNN: Before and After Hyperparameter Tuning\n",
    "\n",
    "# Default KNN\n",
    "knn_default = KNeighborsClassifier()\n",
    "knn_default.fit(X_train_70, Y_train_70)\n",
    "y_pred_knn_default = knn_default.predict(X_test_70)\n",
    "acc_knn_default = accuracy_score(Y_test_70, y_pred_knn_default)\n",
    "\n",
    "# Hyperparameter tuned KNN\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "y_pred_knn_tuned = best_knn_model.predict(X_test_70)\n",
    "acc_knn_tuned = accuracy_score(Y_test_70, y_pred_knn_tuned)\n",
    "\n",
    "# Add to results\n",
    "results.append(['KNN', acc_knn_default, acc_knn_tuned])\n",
    "\n",
    "# Convert results to a DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy (Before Tuning)', 'Accuracy (After Tuning)'])\n",
    "\n",
    "# Display the result table\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d7e0bf0-ca5c-466c-8126-d717ce746829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.974910394265233\n",
      "Test Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "X = data.drop(columns=[\"Is_Allergen\"]) # Feature columns\n",
    "Y = data[\"Is_Allergen\"] # Target column\n",
    "\n",
    "\n",
    "# Split the dataset (70:30 split)\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the best model \n",
    "best_model = XGBClassifier(max_depth=3, learning_rate=0.01, n_estimators=100,subsample= 0.8) \n",
    "\n",
    "# Retrain the model\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(best_model, 'best_xgboost_model.pkl')\n",
    "\n",
    "# Verify the model performance\n",
    "train_accuracy = best_model.score(X_train, Y_train)\n",
    "test_accuracy = best_model.score(X_test, Y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101158d-3793-4fd9-84f7-2d100678df98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
