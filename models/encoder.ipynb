{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b59e426-2e78-4c6e-bd60-34fd68cd8b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3d2e79-9b7f-41ba-9bab-4abb4b12818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Food Product Main Ingredient Sweetener Fat/Oil Seasoning  \\\n",
      "0       Almond Cookies         Almonds     Sugar  Butter     Flour   \n",
      "1       Almond Cookies         Almonds     Sugar  Butter     Flour   \n",
      "2  Chicken Noodle Soup   Chicken broth       NaN     NaN      Salt   \n",
      "3  Chicken Noodle Soup   Chicken broth       NaN     NaN      Salt   \n",
      "4       Cheddar Cheese          Cheese       NaN     NaN      Salt   \n",
      "\n",
      "                Allergens  Price ($)  Customer rating (Out of 5)  \n",
      "0   Almonds, Wheat, Dairy      10.15                         3.1  \n",
      "1   Almonds, Wheat, Dairy       6.17                         4.5  \n",
      "2  Chicken, Wheat, Celery      19.65                         4.1  \n",
      "3  Chicken, Wheat, Celery      17.48                         4.7  \n",
      "4                   Dairy      10.83                         3.7  \n",
      "0    Contains\n",
      "1    Contains\n",
      "2    Contains\n",
      "3    Contains\n",
      "4    Contains\n",
      "Name: Prediction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Split into features and target variable\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction']\n",
    "\n",
    "# Display the first few rows of X and y\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7733a9-5edc-40eb-b57a-03873844bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (399, 8)\n",
      "Shape of y: (399,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f1927d-77b3-4986-9f30-be0c4546e821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (319, 8)\n",
      "Shape of X_test: (80, 8)\n",
      "Shape of y_train: (319,)\n",
      "Shape of y_test: (80,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform the train-test split with an 80:20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26803689-a79e-4198-8d67-5f655a8777dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (279, 8)\n",
      "Shape of X_test: (120, 8)\n",
      "Shape of y_train: (279,)\n",
      "Shape of y_test: (120,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the train-test split with a 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e9bdca-6910-4543-8678-f024d731d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (319, 8)\n",
      "Shape of X_test: (80, 8)\n",
      "Shape of y_train: (319,)\n",
      "Shape of y_test: (80,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the 80:20 train-test split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91280855-90e5-4c07-95ce-e4a46a58d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (279, 8)\n",
      "Shape of X_test: (120, 8)\n",
      "Shape of y_train: (279,)\n",
      "Shape of y_test: (120,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the 70:30 train-test split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c55760a-d1f7-4f84-b318-db53b7567660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7583333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.55        33\n",
      "           1       0.83      0.84      0.83        87\n",
      "\n",
      "    accuracy                           0.76       120\n",
      "   macro avg       0.70      0.69      0.69       120\n",
      "weighted avg       0.76      0.76      0.76       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshi\\AppData\\Local\\Temp\\ipykernel_14940\\2719628696.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Akshi\\AppData\\Local\\Temp\\ipykernel_14940\\2719628696.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Akshi\\AppData\\Local\\Temp\\ipykernel_14940\\2719628696.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Akshi\\AppData\\Local\\Temp\\ipykernel_14940\\2719628696.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col].astype(str))\n",
      "C:\\Users\\Akshi\\AppData\\Local\\Temp\\ipykernel_14940\\2719628696.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[['Price ($)', 'Customer rating (Out of 5)']] = scaler.fit_transform(X[['Price ($)', 'Customer rating (Out of 5)']])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Encode the target variable \"Prediction\" to binary format\n",
    "data['Prediction'] = data['Prediction'].apply(lambda x: 1 if x == \"Contains\" else 0)\n",
    "\n",
    "# Select features and encode categorical variables\n",
    "feature_cols = ['Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Price ($)', 'Customer rating (Out of 5)']\n",
    "X = data[feature_cols]\n",
    "y = data['Prediction']\n",
    "\n",
    "# Apply label encoding to categorical features\n",
    "label_encoders = {}\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Standardize the numeric features\n",
    "scaler = StandardScaler()\n",
    "X[['Price ($)', 'Customer rating (Out of 5)']] = scaler.fit_transform(X[['Price ($)', 'Customer rating (Out of 5)']])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = log_reg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa491467-394b-4159-a44b-6b9a64be568e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (990874830.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Nov 4 different modelling\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Nov 4 different modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edacb03-8876-41fe-ab8e-00ee6d3bb9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.9875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        22\n",
      "           1       1.00      0.98      0.99        58\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.98      0.99      0.98        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 1.0000\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00        80\n",
      "   macro avg       1.00      1.00      1.00        80\n",
      "weighted avg       1.00      1.00      1.00        80\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        22\n",
      "           1       1.00      0.98      0.99        58\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.98      0.99      0.98        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.9875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        22\n",
      "           1       1.00      0.98      0.99        58\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.98      0.99      0.98        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.9000\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.85        22\n",
      "           1       1.00      0.86      0.93        58\n",
      "\n",
      "    accuracy                           0.90        80\n",
      "   macro avg       0.87      0.93      0.89        80\n",
      "weighted avg       0.93      0.90      0.90        80\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Preparing the dataset\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction'].apply(lambda x: 1 if x == 'Contains' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns to preprocess\n",
    "categorical_features = ['Food Product', 'Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Allergens']\n",
    "numeric_features = ['Price ($)', 'Customer rating (Out of 5)']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Create a pipeline with the preprocessor and the model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c90171-851d-4fd7-aa2d-3cad960c7990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Training Accuracy  Testing Accuracy\n",
      "0     Logistic Regression           0.990596            0.9875\n",
      "1           Decision Tree           1.000000            1.0000\n",
      "2           Random Forest           1.000000            0.9875\n",
      "3  Support Vector Machine           0.984326            0.9875\n",
      "4     K-Nearest Neighbors           0.902821            0.9000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Preparing the dataset\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction'].apply(lambda x: 1 if x == 'Contains' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns to preprocess\n",
    "categorical_features = ['Food Product', 'Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Allergens']\n",
    "numeric_features = ['Price ($)', 'Customer rating (Out of 5)']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Create a pipeline with the preprocessor and the model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get training accuracy\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Get testing accuracy\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append results as a dictionary to the list\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Training Accuracy': train_accuracy,\n",
    "        'Testing Accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results in a table\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce07f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: category_encoders in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (2.6.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6472675e-665a-4e85-935d-62af70b2168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__criterion': 'gini', 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10}\n",
      "Best Cross-Validation Score: 0.9842757936507937\n",
      "Test Accuracy of Best Model: 0.9875\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import LeaveOneOutEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Preparing the dataset\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction'].apply(lambda x: 1 if x == 'Contains' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns to preprocess\n",
    "categorical_features = ['Food Product', 'Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Allergens']\n",
    "numeric_features = ['Price ($)', 'Customer rating (Out of 5)']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', LeaveOneOutEncoder(cols=categorical_features), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create a pipeline with preprocessor and classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation Score:\", best_score)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy of Best Model:\", test_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9207dd18-0699-499f-9cd2-c4f7ebb9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Best Model: 0.9875\n",
      "Model saved as 'best_decision_tree_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "'''import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Preparing the dataset\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction'].apply(lambda x: 1 if x == 'Contains' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns to preprocess\n",
    "categorical_features = ['Food Product', 'Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Allergens']\n",
    "numeric_features = ['Price ($)', 'Customer rating (Out of 5)']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', LeaveOneOutEncoder(cols=categorical_features), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for Decision Tree hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create a pipeline with preprocessor and classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy of Best Model:\", test_accuracy)\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model, 'best_decision_tree_model.pkl')\n",
    "print(\"Model saved as 'best_decision_tree_model.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4823000d-0e69-490a-bb98-e5b7564a03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "\n",
    "with open('best_decision_tree_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d65e91-f555-4720-91eb-6fb0b9849243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Best Model: 0.9875\n",
      "Model saved as 'best_decision_tree_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"C:/Users/Akshi/FoodAllergyDetection/Dataset/Allergen_Status_of_Food_Products.csv\")\n",
    "\n",
    "# Preparing the dataset\n",
    "X = data.drop(columns=['Prediction'])\n",
    "y = data['Prediction'].apply(lambda x: 1 if x == 'Contains' else 0)\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns to preprocess\n",
    "categorical_features = ['Food Product', 'Main Ingredient', 'Sweetener', 'Fat/Oil', 'Seasoning', 'Allergens']\n",
    "numeric_features = ['Price ($)', 'Customer rating (Out of 5)']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for Decision Tree hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create a pipeline with preprocessor and classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy of Best Model:\", test_accuracy)\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model, 'best_decision_tree_model.pkl')\n",
    "print(\"Model saved as 'best_decision_tree_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80abd5ee-62b1-45bf-b7f4-e56981c3e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('best_decision_tree_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ae1fa-5bbd-4abd-84d1-a9d7305b9ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
